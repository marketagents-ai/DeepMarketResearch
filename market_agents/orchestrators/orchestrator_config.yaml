# orchestrator_config.yaml

num_agents: 2
max_rounds: 2
environment_order:
#  - group_chat
  - research
tool_mode: true
agent_config:
  knowledge_base: "hamlet_kb"
  use_llm: true
llm_configs:
    - name: "gpt-4o"
      model: "gpt-4o-mini"
      client: "openai"
      max_tokens: 1024
      temperature: 0.5
      use_cache: true
#    - name: "hermes"
#      model: "NousResearch/Hermes-3-Llama-3.1-8B"
#      client: "litellm"
#      max_tokens: 4096
#      temperature: 0.5
#      use_cache: true
#    - name: "qwen"
#      model: "Qwen/Qwen2.5-7B-Instruct"
#      client: "litellm"
#      max_tokens: 4096
#      temperature: 0.5
#      use_cache: true
#    - name: "internlm"
#      model: "internlm/internlm2_5-7b-chat"
#      client: "litellm"
#      max_tokens: 4096
#      temperature: 0.5
#      use_cache: true
#    - name: "mistral"
#      model: "mistralai/Mistral-7B-Instruct-v0.3"
#      client: "litellm"
#      max_tokens: 4096
#      temperature: 0.0
#      use_cache: true
#    - name: "llama"
#      model: "meta-llama/Llama-3.1-8B-Instruct"
#      client: "litellm"
#      max_tokens: 4096
#      temperature: 0.5
#      use_cache: true
#    - name: "functionary"
#      model: "meetkai/functionary-small-v3.1"
#      client: "litellm"
#      max_tokens: 4096
#      temperature: 0.5
#      use_cache: true

environment_configs:
  group_chat:
    name: "group_chat"
    api_url: "http://localhost:8002"
    max_rounds: 5
  #  initial_topic: "Initial Market Discussion"
    initial_topic: "Hamlet's famous 'To be or not to be' soliloquy"
    sub_rounds: 2
    group_size: 4
  research:
    name: "literary_research"
    api_url: "http://localhost:8003"
    max_rounds: 5
#    initial_topic: "Market Analysis"
    initial_topic: "Hamlet's famous 'To be or not to be' soliloquy"
    sub_rounds: 2
    group_size: 4
    schema_model: "LiteraryAnalysis" 
protocol: "acl_message"
database_config:
  db_host: "localhost"
  db_port: "5433"

request_limits:
  openai:
    max_requests_per_minute: 500
    max_tokens_per_minute: 40000
  anthropic:
    max_requests_per_minute: 300
  vllm:
    max_requests_per_minute: 150
    max_tokens_per_minute: 50000
  litellm:
    max_requests_per_minute: 100
    max_tokens_per_minute: 35000